select(
-DENSITY
) |>
na.omit() -> data_model
cv_indices <- sample(1:K, size = nrow(data_model), replace = TRUE)
cv_indices <- sample(1:K, size = nrow(data_model), replace = F)
cv_indices <- rep_len(1:K, nrow(data_model))
cv_indices <- sample(cv_indices, nrow(data_model))
table(cv_indices)
# Define the number of folds for cross-validation
K <- 5
# Create an empty vector to store the cross-validation errors
cv_errors <- numeric(K)
# Create indices for cross-validation folds
set.seed(123) # for reproducibility
cv_indices <- rep_len(1:K, nrow(data_model))
cv_indices <- sample(cv_indices, nrow(data_model))
table(cv_indices)
# Perform cross-validation
k <- 1
# Perform cross-validation
fold <- 1
# Extract training and test sets for this fold
train_data <- data_model[cv_indices != fold, ]
test_data <- data_model[cv_indices == fold, ]
# Fit the model on the training data
full_model <- lm(BODYFAT ~ ., data_model, data = train_data)
# Fit the model on the training data
full_model <- lm(BODYFAT ~ ., data = train_data)
# Reduced the model
reduced_model <- stepAIC(full_model)
library(MASS)
if(!require(tidyverse)) install.packages("tidyverse")
# Reduced the model
reduced_model <- stepAIC(full_model)
# Reduced the model
reduced_model <- stepAIC(full_model,trace = F)
# Predict on the test data
predictions <- predict(reduced_model, newdata = test_data)
# Calculate the mean squared error for this fold
cv_errors[fold] <- mean((predictions - test_data[[response]])^2)
str(test_data)
# Calculate the mean squared error for this fold
cv_errors[fold] <- mean((predictions - test_data[["BODYFAT"]])^2)
# Calculate the mean cross-validation error
mean_cv_error <- mean(cv_errors)
mean_cv_error
# Calculate the mean cross-validation error
boxplot(cv_errors)
cv_errors
# Perform cross-validation
for (fold in 1:K) {
# Extract training and test sets for this fold
train_data <- data_model[cv_indices != fold, ]
test_data <- data_model[cv_indices == fold, ]
# Fit the model on the training data
full_model <- lm(BODYFAT ~ ., data = train_data)
# Reduced the model
reduced_model <- stepAIC(full_model,trace = F)
# Predict on the test data
predictions <- predict(reduced_model, newdata = test_data)
# Calculate the mean squared error for this fold
cv_errors[fold] <- mean((predictions - test_data[["BODYFAT"]])^2)
}
cv_errors
# Calculate the mean cross-validation error
cv_errors
mean_cv_error <- mean(cv_errors)
mean_cv_error
(RSS_n <- mean(bf_mod$residuals^2))
bf_mod <- lm(BODYFAT ~ ., data_model)
summary(bf_mod)
(RSS_n <- mean(bf_mod$residuals^2))
(RSS_n_p <- sum(bf_mod$residuals^2)/
(nrow(data_model) - length(bf_mod$coefficients)))
# Fit the model on the whole data
full_model <- lm(BODYFAT ~ ., data = data_model)
# Reduced the model
reduced_model <- stepAIC(full_model,trace = F)
(RSS_n <- mean(reduced_model$residuals^2))
(RSS_n_p <- sum(reduced_model$residuals^2)/
(nrow(data_model) - length(reduced_model$coefficients)))
# Fit the model on the whole data
full_model <- lm(BODYFAT ~ ., data = data_model)
# Reduced the model
final_model <- stepAIC(full_model,trace = F)
# again why CV is important
(RSS_n <- mean(final_model$residuals^2))
(RSS_n_p <- sum(final_model$residuals^2)/
(nrow(data_model) - length(final_model$coefficients)))
final_model
summary(final_model)
str(data_model)
ggplot(model.resid(final_model), aes(x = fitted(final_model), y = .)) +
geom_point() +
labs(
title = "Residuals vs Fitted Values",
x = "Fitted Values",
y = "Residuals"
)
ggplot(
data = NULL,
aes(x = final_model$fitted.values, y = final_model$residuals)
) +
geom_point() +
labs(
title = "Residuals vs Fitted Values",
x = "Fitted Values",
y = "Residuals"
)
ggplot(
data = NULL,
aes(x = final_model$fitted.values, y = final_model$residuals)
) +
geom_point() +
geom_hline(yintercept = 0) +
labs(
title = "Residuals vs Fitted Values",
x = "Fitted Values",
y = "Residuals"
)
ggplot(
data = NULL,
aes(x = final_model$fitted.values, y = final_model$residuals)
) +
geom_point() +
geom_hline(yintercept = 0, color = "red") +
labs(
title = "Residuals vs Fitted Values",
x = "Fitted Values",
y = "Residuals"
)
ggplot(
data = NULL,
aes(sample = final_model$residuals)) +
stat_qqplot() +
labs(
title = "Normal Q-Q Plot",
x = "Sample Quantiles",
y = "Standard Normal Quantiles"
)
ggplot(
data = NULL,
aes(sample = final_model$residuals)) +
stat_qq() +
stat_qq_line() +
labs(
title = "Normal Q-Q Plot",
x = "Sample Quantiles",
y = "Standard Normal Quantiles"
)
correlation_table <- cor(data)
corrplot(correlation_table, method = 'number') # colorful numbers
library(car) # For VIF calculation
if(!require(car)) install.packages("car")
vif_values <- vif(final_model)
library(car)
vif_values <- vif(final_model)
summary(vif_values)
vif_values
vif_values
library(MASS) # step
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(corrplot)) install.packages("corrplot")
if(!require(car)) install.packages("car") # For VIF calculation
head(data_model)
vif(full_model)
# Split into 70% training and 30% test sets
set.seed(123)  # Set random seed for reproducibility
split_index <- sample.split(data_model$BODYFAT, SplitRatio = 0.7)
# Set seed for reproducibility
set.seed(123)
# Split data into training and test sets
train_index <- createDataPartition(data$BODYFAT, p = 0.7, list = FALSE)
# Split data into training and test sets
library(caret)
install.packages("caret")
# Set seed for reproducibility
set.seed(123)
# Split data into training and test sets
library(caret)
train_index <- createDataPartition(data$BODYFAT, p = 0.7, list = FALSE)
training_data <- data[train_index, ]
testing_data <- data[-train_index, ]
# Set seed for reproducibility
set.seed(123)
# Split data into training and test sets
library(caret)
train_index <- createDataPartition(data_model$BODYFAT, p = 0.7, list = FALSE)
training_data <- data_model[train_index, ]
testing_data <- data_model[-train_index, ]
# Define the number of folds for cross-validation
K <- 5
# Define the number of folds for cross-validation
K <- 10
# Create an empty vector to store the cross-validation errors
cv_errors <- numeric(K)
# Create indices for cross-validation folds
cv_indices <- rep_len(1:K, nrow(training_data))
cv_indices <- sample(cv_indices, nrow(training_data))
table(cv_indices)
fold <- 1
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the model on the training data
full_model_with_weight <- lm(BODYFAT ~ ., data = train_folds)
full_model_no_weight <- lm(BODYFAT ~ . -WEIGHT, data = train_folds)
# Define the number of folds for cross-validation
K <- 10
# Create an empty vector to store the cross-validation errors
cv_errors <- matrix(K,2) # 2 models to compare
# Create indices for cross-validation folds
cv_indices <- rep_len(1:K, nrow(training_data))
cv_indices <- sample(cv_indices, nrow(training_data))
table(cv_indices)
# Perform cross-validation
for (fold in 1:K) {
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the model on the training data
full_model_with_weight <- lm(BODYFAT ~ ., data = train_folds)
full_model_no_weight <- lm(BODYFAT ~ . -WEIGHT, data = train_folds)
# Reduced the model
reduced_model_with_weight <- stepAIC(full_model_with_weight,trace = F)
reduced_model_no_weight <- stepAIC(full_model_no_weight,trace = F)
# Predict on the test data
predictions_with_weight <- predict(reduced_model, newdata = test_fold)
predictions_no_weight <- predict(reduced_model, newdata = test_fold)
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
}
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the model on the training data
full_model_with_weight <- lm(BODYFAT ~ ., data = train_folds)
full_model_no_weight <- lm(BODYFAT ~ . -WEIGHT, data = train_folds)
# Reduced the model
reduced_model_with_weight <- stepAIC(full_model_with_weight,trace = F)
reduced_model_no_weight <- stepAIC(full_model_no_weight,trace = F)
# Predict on the test data
predictions_with_weight <- predict(reduced_model, newdata = test_fold)
predictions_no_weight <- predict(reduced_model, newdata = test_fold)
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
# Create an empty vector to store the cross-validation errors
cv_errors <- matrix(nrow = K,ncol = 2) # 2 models to compare
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
# Calculate the mean cross-validation error
cv_errors
# Define the number of folds for cross-validation
K <- 10
# Create an empty vector to store the cross-validation errors
cv_errors <- matrix(nrow = K,ncol = 2) # 2 models to compare
# Create indices for cross-validation folds
cv_indices <- rep_len(1:K, nrow(training_data))
cv_indices <- sample(cv_indices, nrow(training_data))
table(cv_indices)
# Perform cross-validation
for (fold in 1:K) {
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the model on the training data
full_model_with_weight <- lm(BODYFAT ~ ., data = train_folds)
full_model_no_weight <- lm(BODYFAT ~ . -WEIGHT, data = train_folds)
# Reduced the model
reduced_model_with_weight <- stepAIC(full_model_with_weight,trace = F)
reduced_model_no_weight <- stepAIC(full_model_no_weight,trace = F)
# Predict on the test data
predictions_with_weight <- predict(reduced_model, newdata = test_fold)
predictions_no_weight <- predict(reduced_model, newdata = test_fold)
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
}
# Calculate the mean cross-validation error
cv_errors
mean_cv_error <- colMeans(cv_errors)
mean_cv_error
# Define the number of folds for cross-validation
K <- 10
# Create an empty vector to store the cross-validation errors
cv_errors <- matrix(nrow = K,ncol = 2) # 2 models to compare
# Create indices for cross-validation folds
cv_indices <- rep_len(1:K, nrow(training_data))
cv_indices <- sample(cv_indices, nrow(training_data))
table(cv_indices)
# Perform cross-validation
for (fold in 1:K) {
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the model on the training data
full_model_with_weight <- lm(BODYFAT ~ ., data = train_folds)
full_model_no_weight <- lm(BODYFAT ~ . -WEIGHT, data = train_folds)
# Reduced the model
reduced_model_with_weight <- stepAIC(full_model_with_weight,trace = F)
reduced_model_no_weight <- stepAIC(full_model_no_weight,trace = F)
# Predict on the test data
predictions_with_weight <- predict(reduced_model_with_weight, newdata = test_fold)
predictions_no_weight <- predict(reduced_model_no_weight, newdata = test_fold)
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
}
# Calculate the mean cross-validation error
cv_errors
mean_cv_error <- colMeans(cv_errors)
mean_cv_error
full_model_with_weight
full_model_no_weight
reduced_model_with_weight
reduced_model_no_weight
head(predictions_with_weight)
head(predictions_no_weight)
mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
rm(fold)
# Define the number of folds for cross-validation
K <- 10
# Create an empty vector to store the cross-validation errors
cv_errors <- matrix(nrow = K,ncol = 2) # 2 models to compare
# Create indices for cross-validation folds
cv_indices <- rep_len(1:K, nrow(training_data))
cv_indices <- sample(cv_indices, nrow(training_data))
table(cv_indices)
# Perform cross-validation
for (fold in 1:K) {
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the model on the training data
full_model_with_weight <- lm(BODYFAT ~ ., data = train_folds)
full_model_no_weight <- lm(BODYFAT ~ . -WEIGHT, data = train_folds)
# Reduced the model
reduced_model_with_weight <- stepAIC(full_model_with_weight,trace = F)
reduced_model_no_weight <- stepAIC(full_model_no_weight,trace = F)
# Predict on the test data
predictions_with_weight <- predict(reduced_model_with_weight, newdata = test_fold)
predictions_no_weight <- predict(reduced_model_no_weight, newdata = test_fold)
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
}
# Calculate the mean cross-validation error
cv_errors
mean_cv_error <- colMeans(cv_errors)
mean_cv_error
# Set seed for reproducibility
set.seed(123)
# Split data into training and test sets
library(caret)
train_index <- createDataPartition(data_model$BODYFAT, p = 0.7, list = FALSE)
training_data <- data_model[train_index, ]
testing_data <- data_model[-train_index, ]
# Define the number of folds for cross-validation
K <- 10
# Create an empty vector to store the cross-validation errors
cv_errors <- matrix(nrow = K,ncol = 2) # 2 models to compare
# Create indices for cross-validation folds
cv_indices <- rep_len(1:K, nrow(training_data))
cv_indices <- sample(cv_indices, nrow(training_data))
table(cv_indices)
# Perform cross-validation
for (fold in 1:K) {
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the model on the training data
full_model_with_weight <- lm(BODYFAT ~ ., data = train_folds)
full_model_no_weight <- lm(BODYFAT ~ . -WEIGHT, data = train_folds)
# Reduced the model
reduced_model_with_weight <- stepAIC(full_model_with_weight,trace = F)
reduced_model_no_weight <- stepAIC(full_model_no_weight,trace = F)
# Predict on the test data
predictions_with_weight <- predict(reduced_model_with_weight, newdata = test_fold)
predictions_no_weight <- predict(reduced_model_no_weight, newdata = test_fold)
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
}
# Calculate the mean cross-validation error
cv_errors
mean_cv_error <- colMeans(cv_errors)
mean_cv_error
# Define the number of folds for cross-validation
K <- 10
# Create an empty vector to store the cross-validation errors
cv_errors <- matrix(nrow = K,ncol = 2) # 2 models to compare
# Create indices for cross-validation folds
cv_indices <- rep_len(1:K, nrow(training_data))
cv_indices <- sample(cv_indices, nrow(training_data))
table(cv_indices)
# Perform cross-validation
for (fold in 1:K) {
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the model on the training data
full_model_with_weight <- lm(BODYFAT ~ ., data = train_folds)
full_model_no_weight <- lm(BODYFAT ~ . -WEIGHT, data = train_folds)
# Reduced the model
reduced_model_with_weight <- stepAIC(full_model_with_weight,trace = F)
reduced_model_no_weight <- stepAIC(full_model_no_weight,trace = F)
# Predict on the test data
predictions_with_weight <- predict(reduced_model_with_weight, newdata = test_fold)
predictions_no_weight <- predict(reduced_model_no_weight, newdata = test_fold)
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
}
# Calculate the mean cross-validation error
cv_errors
mean_cv_error <- colMeans(cv_errors)
mean_cv_error
mean((predictions_with_weight - test_fold[["BODYFAT"]])^2)
mean((predictions_no_weight - test_fold[["BODYFAT"]])^2)
head(predictions_with_weight)
head(predictions_no_weight)
reduced_model_with_weight
reduced_model_no_weight
full_model_with_weight
full_model_no_weight
reduced_model_with_weight
reduced_model_no_weight
vif(reduced_model_no_weight)
# Fit the full model on the training folds
full_model <- lm(BODYFAT ~ ., data = train_folds)
# Reduced the model
reduced_model <- stepAIC(full_model,trace = F)
# VIF
vif(reduced_model)
# VIF
vifs <- vif(reduced_model)
which.max(vifs)
variable.names(reduced_model)
variable_to_remove <- variable.names(reduced_model)[which.max(vifs) + 1] # +1 for intercept
variable_to_remove
vif_model <- update(reduced_model, . ~ . - variable_to_remove)
full_model
reduced_model
vif_model
variable_to_remove
vif_model <- update(reduced_model, . ~ . - HIP)
vif_model
vif_model <- update(reduced_model, . ~ . - .(variable_to_remove))
vif_model
vif_model <- update(reduced_model, . ~ . - evaluate(variable_to_remove))
vif_model
vif_model <- update(
reduced_model,
as.formula(paste(". ~ . - ",variable_to_remove))
)
vif_model
# Define the number of folds for cross-validation
K <- 10
# Create an empty vector to store the cross-validation errors
cv_errors <- matrix(nrow = K,ncol = 3) # 4 models to compare
# Create indices for cross-validation folds
cv_indices <- rep_len(1:K, nrow(training_data))
cv_indices <- sample(cv_indices, nrow(training_data))
table(cv_indices)
# Perform cross-validation
for (fold in 1:K) {
# Extract training and test sets for this fold
train_folds <- training_data[cv_indices != fold, ]
test_fold <- training_data[cv_indices == fold, ]
# Fit the full model on the training folds
full_model <- lm(BODYFAT ~ ., data = train_folds)
# Reduced the model
reduced_model <- stepAIC(full_model,trace = F)
# VIF
vifs <- vif(reduced_model)
variable_to_remove <- variable.names(reduced_model)[which.max(vifs) + 1] # +1 for intercept
vif_model <- update(
reduced_model,
as.formula(paste(". ~ . - ",variable_to_remove))
)
# Predict on the test data
predictions_full <- predict(full_model, newdata = test_fold)
predictions_reduced_model <- predict(reduced_model, newdata = test_fold)
predictions_vif <- predict(vif_model, newdata = test_fold)
# Calculate the mean squared error for this fold
cv_errors[fold,1] <- mean((predictions_full - test_fold[["BODYFAT"]])^2)
cv_errors[fold,2] <- mean((predictions_reduced_model - test_fold[["BODYFAT"]])^2)
cv_errors[fold,3] <- mean((predictions_vif - test_fold[["BODYFAT"]])^2)
}
# Calculate the mean cross-validation error
cv_errors
mean_cv_error <- colMeans(cv_errors)
mean_cv_error
vifs
# Fit the full model on the training data
full_model <- lm(BODYFAT ~ ., data = training_data)
reduced_model <- stepAIC(full_model,trace = F)
# Fit the full model on the training data
full_model <- lm(BODYFAT ~ ., data = training_data)
reduced_model <- stepAIC(full_model,trace = F)
predictions_reduced_model <- predict(reduced_model, newdata = testing_data)
mean((predictions_reduced_model - testing_data[["BODYFAT"]])^2)
# Fit the full model on the training data
full_model <- lm(BODYFAT ~ ., data = data_model)
final_model <- stepAIC(final_model,trace = F)
summary(final_model)
VCV
vif_values

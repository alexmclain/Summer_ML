---
title: "Unsupervised ML"
format: docx
editor: source
---

# Libraries

```{r}
library(tidyverse)
library(GGally)

```

# Read data

```{r}
data_to_model <- readRDS(file = "processed_data/data_to_model.rds")

data_to_model |> 
  select_if(is.numeric) |> 
  summary()
```

```{r}
data_to_model |> 
  select(ends_with("scaled")) |> 
  as.matrix() -> numerics_scaled
```


```{r}
numerics_scaled |> 
  ggpairs()
```

# Principal components analysis (PCA)



```{r}
# Perform PCA using princomp
pca_results <- princomp(numerics_scaled)
summary(pca_results)
biplot(pca_results)

```

## Explained variance and scree plot

To decide how many principal components to retain, you can use several criteria. Some commonly used methods:


### Elbow Method

Look for an "elbow" in the scree plot, where the explained variance starts to level off. The principal components to the left of this elbow are typically considered important.

```{r}
# Calculate eigenvalues
eigenvalues <- pca_results$sdev^2
explained_variance <- eigenvalues / sum(eigenvalues) * 100
explained_variance
```

```{r}
# Create a data frame for plotting
scree_data <- data.frame(
  Principal_Component = seq_along(explained_variance),
  explained_variance = explained_variance
)

# Create the scree plot using ggplot2
ggplot(scree_data, aes(x = Principal_Component, y = explained_variance)) +
  geom_line() +
  geom_point() +
  labs(title = "Scree Plot",
       x = "Principal Component",
       y = "Proportion of Variance Explained") +
  theme_minimal()
```
7 seems ok with the scree plot

### Kaiser Criterion

Keep all principal components with eigenvalues (variance explained) greater than 1. This criterion is based on the idea that a principal component should explain at least as much variance as one of the original variables.

```{r}
# Print eigenvalues
print(eigenvalues)
```

We would use 4

### Proportion of Variance Explained

Choose enough components to explain a high cumulative proportion of the variance (e.g., 80% or 90%).

```{r}
cumsum(explained_variance)
```

6 or 7

## Ploting the scores

```{r}
num_scores_retained <- 7
pc_scores <- pca_results$scores[, 1:num_scores_retained]
# Select PCs based on explained variance (optional)
```

```{r}
# Plot the first two principal components
pc_scores |> 
  as.data.frame() |> 
  ggplot( aes(x = Comp.1, y = Comp.2)) +
  geom_point(aes(color = data_to_model$ANY_COMPLICATION)) +
  labs(
    title = "PCA of Myocardial infarction complications",
     x = "Principal Component 1",
     y = "Principal Component 2"
    )
```




# Principal components clustering

Principal components clustering is
simply clustering the data based on
M â‰¤ p principal components. Remeber we do not have to, and that we can use the original data. Here we are first reducing the data with PCA and then applying the clustering technique.

## K-means algorithm

```{r}
# Set the number of clusters
num_clusters <- 3  # Example: 3 clusters

# Perform k-means clustering
set.seed(123)  # Set seed for reproducibility
kmeans_result <- kmeans(pc_scores, centers = num_clusters)

# Print k-means result
print(kmeans_result)
```


```{r}
# Add cluster assignments to the principal components data frame
pca_data <- as.data.frame(pc_scores)
clusters_kmeans <- factor(kmeans_result$cluster)
pca_data$Cluster <- clusters_kmeans

# Plot the first two principal components with cluster assignments
ggplot(pca_data, aes(x = Comp.1, y = Comp.2, color = Cluster)) +
  geom_point(size = 3) +
  labs(title = "K-means Clustering on PCA Results",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal()
```


## Hierarchical Clustering

```{r}
# Compute the distance matrix
distance_matrix <- dist(pc_scores)

# Perform hierarchical clustering
hc_result <- hclust(distance_matrix, method = "ward.D2")

# Plot the dendrogram
plot(hc_result)
```


```{r}
# Cut the dendrogram to form clusters
num_clusters <- 3  # Example: 3 clusters
clusters_hclust <- cutree(hc_result, k = num_clusters)

# Print the clusters
table(clusters_hclust)
```
Notice they do not have to be the same as kmeans

```{r}
table(clusters_kmeans, clusters_hclust)
```

```{r}
# Add cluster assignments to the principal components data frame
pca_data <- as.data.frame(pc_scores)
pca_data$Cluster <- factor(clusters_hclust)

# Plot the first two principal components with cluster assignments
ggplot(pca_data, aes(x = Comp.1, y = Comp.2, color = Cluster)) +
  geom_point(size = 3) +
  labs(title = "Hierarchical Clustering on PCA Results",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal()
```

# Comparison with outcomes

```{r}
table(clusters_kmeans, data_to_model$DIED)
table(clusters_kmeans, data_to_model$DIED) |> 
  prop.table(margin = 1)
```

```{r}
table(clusters_kmeans, data_to_model$ANY_COMPLICATION)
table(clusters_kmeans, data_to_model$ANY_COMPLICATION) |> 
  prop.table(margin = 1)
```

# If you have categorical data

Multiple Correspondence Analysis (MCA): Use MCA for dimension reduction of categorical data.
Hierarchical Clustering on MCA Results: Perform hierarchical clustering on the results of MCA.
k-modes Clustering: Use k-modes for clustering directly on categorical data.
